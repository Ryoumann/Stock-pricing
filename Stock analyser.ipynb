{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Author: Max Mann\n",
    "# Credit: https://www.datacamp.com/community/tutorials/lstm-python-stock-market \n",
    "\n",
    "# Versions:\n",
    "#     pandas_datareader = 0.9.0\n",
    "#     matplotlib = 3.3.3\n",
    "#     pandas = 1.1.4\n",
    "#     numpy = 1.19.2\n",
    "#     tensorflow = 1.15.0\n",
    "  \n",
    "from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import urllib.request, json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_source = 'alphavantage' \n",
    "print(pd.__version__)\n",
    "if data_source == 'alphavantage':\n",
    "    api_key = '5M2D85XZTSR1UZWI'\n",
    "    ticker = \"TSLA\"\n",
    "    file_to_save = '{}_test_stock_market_data.csv'.format(ticker)\n",
    "    \n",
    "    if not os.path.exists(file_to_save):\n",
    "        text_file = open(file_to_save, \"w\")\n",
    "        for i in range(1, 3, 1):\n",
    "            for j in range(1, 13, 1):\n",
    "                year = i\n",
    "                month = j\n",
    "                url_string = \"https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY_EXTENDED&symbol={}&interval=1min&slice=year{}month{}&adjusted=true&apikey={}\".format(ticker, year, month, api_key)\n",
    "                with urllib.request.urlopen(url_string) as url:\n",
    "                    data = url.read().decode()\n",
    "                    print(i, j)\n",
    "                    if i * j == 1:\n",
    "                        text_file.write(data)\n",
    "                    else:\n",
    "                        text_file.write(data[32:])\n",
    "        text_file.close()\n",
    "     \n",
    "    #colnames=['Time','Open','High','Low','Close','Volume'] \n",
    " \n",
    "    df = pd.read_csv(file_to_save)\n",
    "    df = df.rename(columns = {\"time\":\"Time\", \"open\":\"Open\", \"high\":\"High\", \"low\":\"Low\", \"close\":\"Close\", \"volume\":\"Volume\"})\n",
    "    #df[\"Time\"]=pd.to_datetime(df[\"Time\"], format = '%Y%m%d')\n",
    "    df[\"Time\"]=df[\"Time\"].astype('datetime64[ns]')\n",
    "    df = df.reindex(index=df.index[::-1], method = 'backfill')\n",
    "    \n",
    "    \n",
    "    display(df.head())\n",
    "    print(df.dtypes)\n",
    "    print(file_to_save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,9))\n",
    "plt.plot(range(len(df.index)),(df['Low']+df['High'])/2)\n",
    "plt.xticks(range(0,df.shape[0],25000),df['Time'].loc[::5000],rotation=45)\n",
    "plt.xlabel('Time',fontsize=18)\n",
    "plt.ylabel('Mid Price',fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "high_prices = df.loc[:,'High'].values\n",
    "low_prices = df.loc[:,'Low'].values\n",
    "mid_prices = (high_prices+low_prices)/2.0\n",
    "\n",
    "train_data = mid_prices[:50000]\n",
    "test_data = mid_prices[50000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data to be between 0 and 1\n",
    "# When scaling remember! You normalize both test and train data with respect to training data\n",
    "# Because you are not supposed to have access to test data\n",
    "scaler = MinMaxScaler()\n",
    "train_data = train_data.reshape(-1,1)\n",
    "test_data = test_data.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Scaler with training data and smooth data\n",
    "smoothing_window_size = 2500\n",
    "for di in range(0,10000,smoothing_window_size):\n",
    "    scaler.fit(train_data[di:di+smoothing_window_size,:])\n",
    "    train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])\n",
    "\n",
    "# You normalize the last bit of remaining data\n",
    "scaler.fit(train_data[di+smoothing_window_size:,:])\n",
    "train_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape both train and test data\n",
    "train_data = train_data.reshape(-1)\n",
    "\n",
    "# Normalize test data\n",
    "test_data = scaler.transform(test_data).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now perform exponential moving average smoothing\n",
    "# So the data will have a smoother curve than the original ragged data\n",
    "EMA = 0.0\n",
    "gamma = 0.1\n",
    "for ti in range(11000):\n",
    "  EMA = gamma*train_data[ti] + (1-gamma)*EMA\n",
    "  train_data[ti] = EMA\n",
    "\n",
    "# Used for visualization and test purposes\n",
    "all_mid_data = np.concatenate([train_data,test_data],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "N = train_data.size\n",
    "std_avg_predictions = []\n",
    "std_avg_x = []\n",
    "mse_errors = []\n",
    "k = len(df.index)\n",
    "\n",
    "for pred_idx in range(window_size,N):\n",
    "\n",
    "    if pred_idx >= N:\n",
    "        date = dt.datetime.strptime(k, \"%d/%m/%Y %H:%M:%S\").date() + dt.timedelta(days=1)\n",
    "    else:\n",
    "        time = df.loc[pred_idx,'Time']\n",
    "\n",
    "    std_avg_predictions.append(np.mean(train_data[pred_idx-window_size:pred_idx]))\n",
    "    mse_errors.append((std_avg_predictions[-1]-train_data[pred_idx])**2)\n",
    "    std_avg_x.append(time)\n",
    "\n",
    "print('MSE error for standard averaging: %.5f'%(0.5*np.mean(mse_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,9))\n",
    "plt.plot(range(len(df.index)),all_mid_data,color='b',label='True')\n",
    "plt.plot(range(window_size,N),std_avg_predictions,color='orange',label='Prediction')\n",
    "#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mid Price')\n",
    "plt.legend(fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exponential moving average\n",
    "window_size = 100\n",
    "N = train_data.size\n",
    "\n",
    "run_avg_predictions = []\n",
    "run_avg_x = []\n",
    "\n",
    "mse_errors = []\n",
    "\n",
    "running_mean = 0.0\n",
    "run_avg_predictions.append(running_mean)\n",
    "\n",
    "decay = 0.5\n",
    "\n",
    "for pred_idx in range(1,N):\n",
    "\n",
    "    running_mean = running_mean*decay + (1.0-decay)*train_data[pred_idx-1]\n",
    "    run_avg_predictions.append(running_mean)\n",
    "    mse_errors.append((run_avg_predictions[-1]-train_data[pred_idx])**2)\n",
    "    run_avg_x.append(time)\n",
    "\n",
    "print('MSE error for EMA averaging: %.5f'%(0.5*np.mean(mse_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize = (18,9))\n",
    "plt.plot(range(len(df.index)),all_mid_data,color='b',label='True')\n",
    "plt.plot(range(0,N),run_avg_predictions,color='orange', label='Prediction')\n",
    "#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mid Price')\n",
    "plt.legend(fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneratorSeq(object):\n",
    "\n",
    "    def __init__(self,prices,batch_size,num_unroll):\n",
    "        self._prices = prices\n",
    "        self._prices_length = len(self._prices) - num_unroll\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unroll = num_unroll\n",
    "        self._segments = self._prices_length //self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "\n",
    "    def next_batch(self):\n",
    "\n",
    "        batch_data = np.zeros((self._batch_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size),dtype=np.float32)\n",
    "\n",
    "        for b in range(self._batch_size):\n",
    "            if self._cursor[b]+1>=self._prices_length:\n",
    "                #self._cursor[b] = b * self._segments\n",
    "                self._cursor[b] = np.random.randint(0,(b+1)*self._segments)\n",
    "\n",
    "            batch_data[b] = self._prices[self._cursor[b]]\n",
    "            batch_labels[b]= self._prices[self._cursor[b]+np.random.randint(0,5)]\n",
    "\n",
    "            self._cursor[b] = (self._cursor[b]+1)%self._prices_length\n",
    "\n",
    "        return batch_data,batch_labels\n",
    "\n",
    "    def unroll_batches(self):\n",
    "\n",
    "        unroll_data,unroll_labels = [],[]\n",
    "        init_data, init_label = None,None\n",
    "        for ui in range(self._num_unroll):\n",
    "\n",
    "            data, labels = self.next_batch()    \n",
    "\n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "\n",
    "        return unroll_data, unroll_labels\n",
    "\n",
    "    def reset_indices(self):\n",
    "        for b in range(self._batch_size):\n",
    "            self._cursor[b] = np.random.randint(0,min((b+1)*self._segments,self._prices_length-1))\n",
    "\n",
    "\n",
    "\n",
    "dg = DataGeneratorSeq(train_data,5,5)\n",
    "u_data, u_labels = dg.unroll_batches()\n",
    "\n",
    "for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):   \n",
    "    print('\\n\\nUnrolled index %d'%ui)\n",
    "    dat_ind = dat\n",
    "    lbl_ind = lbl\n",
    "    print('\\tInputs: ',dat )\n",
    "    print('\\n\\tOutput:',lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 1 # Dimensionality of the data. Since your data is 1-D this would be 1\n",
    "num_unrollings = 50 # Number of time steps you look into the future.\n",
    "batch_size = 500 # Number of samples in a batch\n",
    "num_nodes = [200,200,150] # Number of hidden nodes in each layer of the deep LSTM stack we're using\n",
    "n_layers = len(num_nodes) # number of layers\n",
    "dropout = 0.2 # dropout amount\n",
    "\n",
    "tf.compat.v1.reset_default_graph() # This is important in case you run this multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data.\n",
    "train_inputs, train_outputs = [],[]\n",
    "tf.compat.v1.disable_eager_execution() #MIGHT CAUSE ISSUES!!!\n",
    "\n",
    "# You unroll the input over time defining placeholders for each time step\n",
    "for ui in range(num_unrollings):\n",
    "    train_inputs.append(tf.compat.v1.placeholder(tf.float32, shape=[batch_size,D],name='train_inputs_%d'%ui))\n",
    "    train_outputs.append(tf.compat.v1.placeholder(tf.float32, shape=[batch_size,1], name = 'train_outputs_%d'%ui))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cells = [\n",
    "    tf.contrib.rnn.LSTMCell(num_units=num_nodes[li],\n",
    "                            state_is_tuple=True,\n",
    "                            initializer= tf.contrib.layers.xavier_initializer()\n",
    "                           )\n",
    " for li in range(n_layers)]\n",
    "\n",
    "drop_lstm_cells = [tf.contrib.rnn.DropoutWrapper(\n",
    "    lstm, input_keep_prob=1.0,output_keep_prob=1.0-dropout, state_keep_prob=1.0-dropout\n",
    ") for lstm in lstm_cells]\n",
    "drop_multi_cell = tf.contrib.rnn.MultiRNNCell(drop_lstm_cells)\n",
    "multi_cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)\n",
    "\n",
    "w = tf.get_variable('w',shape=[num_nodes[-1], 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.get_variable('b',initializer=tf.random_uniform([1],-0.1,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cell state and hidden state variables to maintain the state of the LSTM\n",
    "c, h = [],[]\n",
    "initial_state = []\n",
    "for li in range(n_layers):\n",
    "  c.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\n",
    "  h.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\n",
    "  initial_state.append(tf.contrib.rnn.LSTMStateTuple(c[li], h[li]))\n",
    "\n",
    "# Do several tensor transofmations, because the function dynamic_rnn requires the output to be of\n",
    "# a specific format. Read more at: https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
    "all_inputs = tf.concat([tf.expand_dims(t,0) for t in train_inputs],axis=0)\n",
    "\n",
    "# all_outputs is [seq_length, batch_size, num_nodes]\n",
    "all_lstm_outputs, state = tf.nn.dynamic_rnn(\n",
    "    drop_multi_cell, all_inputs, initial_state=tuple(initial_state),\n",
    "    time_major = True, dtype=tf.float32)\n",
    "\n",
    "all_lstm_outputs = tf.reshape(all_lstm_outputs, [batch_size*num_unrollings,num_nodes[-1]])\n",
    "\n",
    "all_outputs = tf.nn.xw_plus_b(all_lstm_outputs,w,b)\n",
    "\n",
    "split_outputs = tf.split(all_outputs,num_unrollings,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When calculating the loss you need to be careful about the exact form, because you calculate\n",
    "# loss of all the unrolled steps at the same time\n",
    "# Therefore, take the mean error or each batch and get the sum of that over all the unrolled steps\n",
    "\n",
    "print('Defining training Loss')\n",
    "loss = 0.0\n",
    "with tf.control_dependencies([tf.assign(c[li], state[li][0]) for li in range(n_layers)]+\n",
    "                             [tf.assign(h[li], state[li][1]) for li in range(n_layers)]):\n",
    "  for ui in range(num_unrollings):\n",
    "    loss += tf.reduce_mean(0.5*(split_outputs[ui]-train_outputs[ui])**2)\n",
    "\n",
    "print('Learning rate decay operations')\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "inc_gstep = tf.assign(global_step,global_step + 1)\n",
    "tf_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)\n",
    "tf_min_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)\n",
    "\n",
    "learning_rate = tf.maximum(\n",
    "    tf.train.exponential_decay(tf_learning_rate, global_step, decay_steps=1, decay_rate=0.5, staircase=True),\n",
    "    tf_min_learning_rate)\n",
    "\n",
    "# Optimizer.\n",
    "print('TF Optimization operations')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v))\n",
    "\n",
    "print('\\tAll done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Defining prediction related TF functions')\n",
    "\n",
    "sample_inputs = tf.placeholder(tf.float32, shape=[1,D])\n",
    "\n",
    "# Maintaining LSTM state for prediction stage\n",
    "sample_c, sample_h, initial_sample_state = [],[],[]\n",
    "for li in range(n_layers):\n",
    "  sample_c.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n",
    "  sample_h.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n",
    "  initial_sample_state.append(tf.contrib.rnn.LSTMStateTuple(sample_c[li],sample_h[li]))\n",
    "\n",
    "reset_sample_states = tf.group(*[tf.assign(sample_c[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)],\n",
    "                               *[tf.assign(sample_h[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)])\n",
    "\n",
    "sample_outputs, sample_state = tf.nn.dynamic_rnn(multi_cell, tf.expand_dims(sample_inputs,0),\n",
    "                                   initial_state=tuple(initial_sample_state),\n",
    "                                   time_major = True,\n",
    "                                   dtype=tf.float32)\n",
    "\n",
    "with tf.control_dependencies([tf.assign(sample_c[li],sample_state[li][0]) for li in range(n_layers)]+\n",
    "                              [tf.assign(sample_h[li],sample_state[li][1]) for li in range(n_layers)]):  \n",
    "  sample_prediction = tf.nn.xw_plus_b(tf.reshape(sample_outputs,[1,-1]), w, b)\n",
    "\n",
    "print('\\tAll done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "valid_summary = 1 # Interval you make test predictions\n",
    "\n",
    "n_predict_once = 50 # Number of steps you continously predict for\n",
    "\n",
    "train_seq_length = train_data.size # Full length of the training data\n",
    "\n",
    "train_mse_ot = [] # Accumulate Train losses\n",
    "test_mse_ot = [] # Accumulate Test loss\n",
    "predictions_over_time = [] # Accumulate predictions\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Used for decaying learning rate\n",
    "loss_nondecrease_count = 0\n",
    "loss_nondecrease_threshold = 2 # If the test error hasn't increased in this many steps, decrease learning rate\n",
    "\n",
    "print('Initialized')\n",
    "average_loss = 0\n",
    "\n",
    "# Define data generator\n",
    "data_gen = DataGeneratorSeq(train_data,batch_size,num_unrollings)\n",
    "\n",
    "x_axis_seq = []\n",
    "\n",
    "# Points you start your test predictions from\n",
    "test_points_seq = np.arange(11000,12000,50).tolist()\n",
    "\n",
    "for ep in range(epochs):       \n",
    "\n",
    "    # ========================= Training =====================================\n",
    "    for step in range(train_seq_length//batch_size):\n",
    "\n",
    "        u_data, u_labels = data_gen.unroll_batches()\n",
    "\n",
    "        feed_dict = {}\n",
    "        for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
    "            feed_dict[train_inputs[ui]] = dat.reshape(-1,1)\n",
    "            feed_dict[train_outputs[ui]] = lbl.reshape(-1,1)\n",
    "\n",
    "        feed_dict.update({tf_learning_rate: 0.0001, tf_min_learning_rate:0.000001})\n",
    "\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "\n",
    "        average_loss += l\n",
    "\n",
    "    # ============================ Validation ==============================\n",
    "    if (ep+1) % valid_summary == 0:\n",
    "\n",
    "      average_loss = average_loss/(valid_summary*(train_seq_length//batch_size))\n",
    "\n",
    "      # The average loss\n",
    "      if (ep+1)%valid_summary==0:\n",
    "        print('Average loss at step %d: %f' % (ep+1, average_loss))\n",
    "\n",
    "      train_mse_ot.append(average_loss)\n",
    "\n",
    "      average_loss = 0 # reset loss\n",
    "\n",
    "      predictions_seq = []\n",
    "\n",
    "      mse_test_loss_seq = []\n",
    "\n",
    "      # ===================== Updating State and Making Predicitons ========================\n",
    "      for w_i in test_points_seq:\n",
    "        mse_test_loss = 0.0\n",
    "        our_predictions = []\n",
    "\n",
    "        if (ep+1)-valid_summary==0:\n",
    "          # Only calculate x_axis values in the first validation epoch\n",
    "          x_axis=[]\n",
    "\n",
    "        # Feed in the recent past behavior of stock prices\n",
    "        # to make predictions from that point onwards\n",
    "        for tr_i in range(w_i-num_unrollings+1,w_i-1):\n",
    "          current_price = all_mid_data[tr_i]\n",
    "          feed_dict[sample_inputs] = np.array(current_price).reshape(1,1)    \n",
    "          _ = session.run(sample_prediction,feed_dict=feed_dict)\n",
    "\n",
    "        feed_dict = {}\n",
    "\n",
    "        current_price = all_mid_data[w_i-1]\n",
    "\n",
    "        feed_dict[sample_inputs] = np.array(current_price).reshape(1,1)\n",
    "\n",
    "        # Make predictions for this many steps\n",
    "        # Each prediction uses previous prediciton as it's current input\n",
    "        for pred_i in range(n_predict_once):\n",
    "\n",
    "          pred = session.run(sample_prediction,feed_dict=feed_dict)\n",
    "\n",
    "          our_predictions.append(np.asscalar(pred))\n",
    "\n",
    "          feed_dict[sample_inputs] = np.asarray(pred).reshape(-1,1)\n",
    "\n",
    "          if (ep+1)-valid_summary==0:\n",
    "            # Only calculate x_axis values in the first validation epoch\n",
    "            x_axis.append(w_i+pred_i)\n",
    "\n",
    "          mse_test_loss += 0.5*(pred-all_mid_data[w_i+pred_i])**2\n",
    "\n",
    "        session.run(reset_sample_states)\n",
    "\n",
    "        predictions_seq.append(np.array(our_predictions))\n",
    "\n",
    "        mse_test_loss /= n_predict_once\n",
    "        mse_test_loss_seq.append(mse_test_loss)\n",
    "\n",
    "        if (ep+1)-valid_summary==0:\n",
    "          x_axis_seq.append(x_axis)\n",
    "\n",
    "      current_test_mse = np.mean(mse_test_loss_seq)\n",
    "\n",
    "      # Learning rate decay logic\n",
    "      if len(test_mse_ot)>0 and current_test_mse > min(test_mse_ot):\n",
    "          loss_nondecrease_count += 1\n",
    "      else:\n",
    "          loss_nondecrease_count = 0\n",
    "\n",
    "      if loss_nondecrease_count > loss_nondecrease_threshold :\n",
    "            session.run(inc_gstep)\n",
    "            loss_nondecrease_count = 0\n",
    "            print('\\tDecreasing learning rate by 0.5')\n",
    "\n",
    "      test_mse_ot.append(current_test_mse)\n",
    "      print('\\tTest MSE: %.5f'%np.mean(mse_test_loss_seq))\n",
    "      predictions_over_time.append(predictions_seq)\n",
    "      print('\\tFinished Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prediction_epoch = 28 # replace this with the epoch that you got the best results when running the plotting code\n",
    "\n",
    "plt.figure(figsize = (18,18))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(range(df.shape[0]),all_mid_data,color='b')\n",
    "\n",
    "# Plotting how the predictions change over time\n",
    "# Plot older predictions with low alpha and newer predictions with high alpha\n",
    "start_alpha = 0.25\n",
    "alpha  = np.arange(start_alpha,1.1,(1.0-start_alpha)/len(predictions_over_time[::3]))\n",
    "for p_i,p in enumerate(predictions_over_time[::3]):\n",
    "    for xval,yval in zip(x_axis_seq,p):\n",
    "        plt.plot(xval,yval,color='r',alpha=alpha[p_i])\n",
    "\n",
    "plt.title('Evolution of Test Predictions Over Time',fontsize=18)\n",
    "plt.xlabel('Date',fontsize=18)\n",
    "plt.ylabel('Mid Price',fontsize=18)\n",
    "plt.xlim(11000,12500)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "\n",
    "# Predicting the best test prediction you got\n",
    "plt.plot(range(df.shape[0]),all_mid_data,color='b')\n",
    "for xval,yval in zip(x_axis_seq,predictions_over_time[best_prediction_epoch]):\n",
    "    plt.plot(xval,yval,color='r')\n",
    "\n",
    "plt.title('Best Test Predictions Over Time',fontsize=18)\n",
    "plt.xlabel('Date',fontsize=18)\n",
    "plt.ylabel('Mid Price',fontsize=18)\n",
    "plt.xlim(11000,12500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
